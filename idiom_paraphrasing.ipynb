{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bystrowska/idiom-paraphrasing/blob/main/idiom_paraphrasing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set-up\n",
        "\n",
        "Clone the git repository and install git-lfs to be able to download model files."
      ],
      "metadata": {
        "id": "FGCgCwYK-yEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! sudo apt-get install git-lfs"
      ],
      "metadata": {
        "id": "X7QNUIJveRV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/bystrowska/idiom-paraphrasing.git"
      ],
      "metadata": {
        "id": "0En-oOwA5OMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd idiom-paraphrasing\n",
        "! git lfs install\n",
        "! git lfs pull"
      ],
      "metadata": {
        "id": "LoSBauVbeUiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference\n",
        "\n",
        "This section will use the pre-trained T5-small model saved in the GitHub repository to generate paraphrases from a sample sentene"
      ],
      "metadata": {
        "id": "3SrtIqUPHJZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers"
      ],
      "metadata": {
        "id": "Ffl-qrI7MZKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"t5-small\"\n",
        "sentence = \"paraphrase: There is Peter with a tray of food, it is a sight for sore eyes!\""
      ],
      "metadata": {
        "id": "f4eQuBIXL1Ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, TFT5ForConditionalGeneration\n",
        "\n",
        "model = TFT5ForConditionalGeneration.from_pretrained(\"./models/\" + checkpoint)\n",
        "# model = TFT5ForConditionalGeneration.from_pretrained(path + \"models/\" + checkpoint + \"/tf_model.h5\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "input_ids = tokenizer(sentence, return_tensors=\"tf\").input_ids\n",
        "outputs = model.generate(input_ids)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "pC6-5en3HNXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYuekeXFMeul"
      },
      "source": [
        "# Dataset processing and tokenizing\n",
        "\n",
        "This section will load the PIE dataset from a csv file, create train/test/validate split and tokenize it."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"t5-small\""
      ],
      "metadata": {
        "id": "ajg6mFuQCwYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install datasets transformers"
      ],
      "metadata": {
        "id": "Kynh4kjA8s5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-mSKZOxPFpi"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset('csv', data_files=(\"data.csv\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a 60:20:20 train/test/validate split"
      ],
      "metadata": {
        "id": "iSZAw8R-9DUt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsOHozClRBev"
      },
      "outputs": [],
      "source": [
        "split_dataset = dataset['train'].train_test_split(test_size=0.2)\n",
        "tmp = split_dataset['train'].train_test_split(test_size=0.25)\n",
        "split_dataset['train'] = tmp['train']\n",
        "split_dataset['validate'] = tmp['test']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJLFR9GXexWQ"
      },
      "source": [
        "## Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04F-bMCTfejk"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57ANP1cGJ1J1"
      },
      "outputs": [],
      "source": [
        "prefix = \"paraphrase: \"\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + ex for ex in examples[\"Idiomatic_Sent\"]]\n",
        "    targets = examples[\"Literal_Sent\"]\n",
        "\n",
        "    model_inputs = tokenizer(inputs)\n",
        "\n",
        "    labels = tokenizer(targets).input_ids\n",
        "\n",
        "    model_inputs[\"labels\"] = labels\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3bp-POHOHET"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = split_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=split_dataset[\"train\"].column_names,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets.save_to_disk(\"tokenized_dataset\")"
      ],
      "metadata": {
        "id": "MHx4dgcA-VZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "\n",
        "This section will prepare the toknized dataset and then use it to train the model with AdaFactor optimizer. Afterwards it'll compute evaluation metrics."
      ],
      "metadata": {
        "id": "h6vw7UfS7uAG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VE99HpAXIyPo"
      },
      "outputs": [],
      "source": [
        "checkpoint = \"t5-small\" # t5-small, t5-base or t5-large\n",
        "batch_size = 128\n",
        "num_epochs = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94__aDEAIWFb"
      },
      "outputs": [],
      "source": [
        "!pip install -U nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "um6iApcxQtMN"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers rouge_score sacrebleu sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tgon7RNMQ0p8"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8s3RBs9EQ8Kq"
      },
      "outputs": [],
      "source": [
        "from datasets import load_from_disk\n",
        "\n",
        "tokenized_dataset = load_from_disk(\"tokenized_dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STQUs0yOQ_QU"
      },
      "outputs": [],
      "source": [
        "from transformers import TFT5ForConditionalGeneration\n",
        "\n",
        "model = TFT5ForConditionalGeneration.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoNBP6WlRD-7"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\") \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxRuXuoVRH4T"
      },
      "outputs": [],
      "source": [
        "tf_train_dataset = tokenized_dataset[\"train\"].to_tf_dataset(columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
        "                                                  collate_fn=data_collator,\n",
        "                                                  shuffle=True,\n",
        "                                                  batch_size=batch_size)\n",
        "\n",
        "tf_validate_dataset = tokenized_dataset[\"validate\"].to_tf_dataset(columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
        "                                                  collate_fn=data_collator,\n",
        "                                                  shuffle=False,\n",
        "                                                  batch_size=batch_size)\n",
        "                                                  \n",
        "tf_test_dataset = tokenized_dataset[\"test\"].to_tf_dataset(columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
        "                                                  collate_fn=data_collator,\n",
        "                                                  shuffle=False,\n",
        "                                                  batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbKwAKpgRPGz"
      },
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "bleu = load_metric(\"sacrebleu\")\n",
        "rouge = load_metric(\"rouge\")\n",
        "meteor = load_metric(\"meteor\")\n",
        "sari = load_metric(\"sari\")\n",
        "perplexity = load_metric(\"perplexity\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TGIgAEaBdQ-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics():\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_inputs = []\n",
        "    for batch in tf_test_dataset:\n",
        "        predictions = model.generate(\n",
        "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
        "        )\n",
        "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "        labels = batch[\"labels\"].numpy()\n",
        "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "        decoded_preds = [pred.strip() for pred in decoded_preds]\n",
        "        decoded_labels = [[label.strip()] for label in decoded_labels]\n",
        "        decoded_inputs = tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens=True)\n",
        "        decoded_inputs = [input.strip()[12:] for input in decoded_inputs]\n",
        "        all_preds.extend(decoded_preds)\n",
        "        all_labels.extend(decoded_labels)\n",
        "        all_inputs.extend(decoded_inputs)\n",
        "\n",
        "    results_bleu = bleu.compute(predictions=all_preds, references=all_labels)['score']\n",
        "    print(results_bleu)\n",
        "    results_rouge = {key: value.mid.fmeasure * 100 for key, value in rouge.compute(predictions=all_preds, references=all_labels).items()}\n",
        "    print(results_rouge)\n",
        "    results_meteor = meteor.compute(predictions=all_preds, references=all_labels)['meteor']\n",
        "    print(results_meteor)\n",
        "    results_sari = sari.compute(sources=all_inputs, predictions=all_preds, references=all_labels)['sari']\n",
        "    print(results_sari)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AdaFactor"
      ],
      "metadata": {
        "id": "LBWro57Dw4ku"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bP1XZtAVBRQT"
      },
      "outputs": [],
      "source": [
        "!pip install tensor2tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pCf1ax3BEsB"
      },
      "outputs": [],
      "source": [
        "from transformers import create_optimizer\n",
        "import tensorflow as tf\n",
        "from tensor2tensor.utils.adafactor import AdafactorOptimizer\n",
        "\n",
        "optimizer = AdafactorOptimizer(multiply_by_parameter_scale=False,\n",
        "                               learning_rate=0.001,\n",
        "                               decay_rate=None,\n",
        "                               beta1=0.0,\n",
        "                               clipping_threshold=1.0,\n",
        "                               factored=True,\n",
        "                               simulated_quantize_bits=None,\n",
        "                               parameter_encoding=None,\n",
        "                               use_locking=False,\n",
        "                               epsilon1=1e-30,\n",
        "                               epsilon2=1e-3)\n",
        "\n",
        "num_train_steps = len(tf_train_dataset) * num_epochs\n",
        "\n",
        "model.compile(optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AdamW"
      ],
      "metadata": {
        "id": "grtj_4Ddw1s2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9I_sQkAmEDOB"
      },
      "outputs": [],
      "source": [
        "# from transformers import create_optimizer\n",
        "# import tensorflow as tf\n",
        "\n",
        "# num_train_steps = len(tf_train_dataset) * num_epochs\n",
        "\n",
        "# optimizer, schedule = create_optimizer(\n",
        "#     init_lr=3e-4,\n",
        "#     num_warmup_steps=0,\n",
        "#     num_train_steps=num_train_steps,\n",
        "#     weight_decay_rate=0.01,\n",
        "# )\n",
        "# model.compile(optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training loop"
      ],
      "metadata": {
        "id": "r29DMwknAdE1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTw42rs1Kfbt"
      },
      "outputs": [],
      "source": [
        "history = model.fit(tf_train_dataset,\n",
        "                    validation_data=tf_validate_dataset,\n",
        "                    epochs=num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5Z8Z3j9Xc4J"
      },
      "outputs": [],
      "source": [
        "print(history.params)\n",
        "print(history.history.keys())\n",
        "for key in history.history.keys():\n",
        "  print(str(key) + \": \" + str(history.history[key]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcOP7UVKK2zX"
      },
      "outputs": [],
      "source": [
        "compute_metrics()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "2U0ekkwZQhtx",
        "bUHHVkapMXGO",
        "IYuekeXFMeul",
        "BJLFR9GXexWQ",
        "yqPy458hZkxn",
        "bkYi9B-7dU2W",
        "uzeqXibHBSrI",
        "grtj_4Ddw1s2",
        "LBWro57Dw4ku"
      ],
      "machine_shape": "hm",
      "name": "idiom-paraphrasing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOgMQAal45rn02OlN6GvFfe",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}